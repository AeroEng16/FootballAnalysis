{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYe8bsQZOkWH6OD1xHn7rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AeroEng16/FootballAnalysis/blob/main/naryaHomoraphyRecreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to recreate\n",
        "# https://github.com/DonsetPG/narya/blob/master/narya/tracker/homography_estimator.py\n",
        "\n",
        "\n",
        "CHECK IF KERAS VERSION IS THE PROBLEM\n",
        "\n",
        "!pip install segmentation-models\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "\n",
        "#import mxnet as mx\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "from tensorflow import keras\n",
        "import segmentation_models as sm\n",
        "\n",
        "\n",
        "#from gluoncv.data.transforms.presets.ssd import transform_test\n",
        "\n",
        "#import mxnet as mx\n",
        "import tensorflow as tf\n",
        "\n",
        "#RESNET_ARCHI_TF_KERAS_PATH = (\n",
        "#    \"https://storage.googleapis.com/narya-bucket-1/models/deep_homo_model_1.h5\"\n",
        "#)\n",
        "RESNET_ARCHI_TF_KERAS_PATH = \"/content/deep_homo_model_1.h5\"\n",
        "RESNET_ARCHI_TF_KERAS_NAME = \"deep_homo_model_1.h5\"\n",
        "RESNET_ARCHI_TF_KERAS_TOTAR = False"
      ],
      "metadata": {
        "id": "PdwMjarvIa3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baebf59b-8e09-4b9e-968b-210299e90293"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.11/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.11/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.12.1)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/narya-bucket-1/models/deep_homo_model_1.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm_eZzwfZjW6",
        "outputId": "d4766da2-0c15-407f-da3f-5cf93af6829d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-14 12:20:02--  https://storage.googleapis.com/narya-bucket-1/models/deep_homo_model_1.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.98.207, 74.125.197.207, 74.125.135.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.98.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44976464 (43M) [application/octet-stream]\n",
            "Saving to: ‘deep_homo_model_1.h5’\n",
            "\n",
            "deep_homo_model_1.h 100%[===================>]  42.89M   200MB/s    in 0.2s    \n",
            "\n",
            "2025-02-14 12:20:02 (200 MB/s) - ‘deep_homo_model_1.h5’ saved [44976464/44976464]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def to_numpy(var):\n",
        "    \"\"\"Parse a Torch variable to a numpy array\n",
        "\n",
        "    Arguments:\n",
        "        var: torch variable\n",
        "    Returns:\n",
        "        a np.array with the same value as var\n",
        "    Raises:\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return var.numpy()\n",
        "    except:\n",
        "        return var.detach().numpy()\n",
        "\n",
        "\n",
        "def to_torch(np_array):\n",
        "    \"\"\"Parse a numpy array to a torch variable\n",
        "\n",
        "    Arguments:\n",
        "        np_array: a np.array\n",
        "    Returns:\n",
        "        a torch Var with the same value as the np_array\n",
        "    Raises:\n",
        "\n",
        "    \"\"\"\n",
        "    tensor = torch.from_numpy(np_array).float()\n",
        "    return torch.autograd.Variable(tensor, requires_grad=False)\n",
        "\n",
        "def torch_img_to_np_img(torch_img):\n",
        "    \"\"\"Convert a torch image to a numpy image\n",
        "\n",
        "    Arguments:\n",
        "        torch_img: Tensor of shape (B,C,H,W) or (C,H,W)\n",
        "    Returns:\n",
        "        a np.array of shape (B,H,W,C) or (H,W,C)\n",
        "    Raises:\n",
        "        ValueError: If this is not a Torch tensor\n",
        "    \"\"\"\n",
        "    if isinstance(torch_img, np.ndarray):\n",
        "        return torch_img\n",
        "    assert isinstance(torch_img, torch.Tensor), \"cannot process data type: {0}\".format(\n",
        "        type(torch_img)\n",
        "    )\n",
        "    if len(torch_img.shape) == 4 and (\n",
        "        torch_img.shape[1] == 3 or torch_img.shape[1] == 1\n",
        "    ):\n",
        "        return np.transpose(torch_img.detach().cpu().numpy(), (0, 2, 3, 1))\n",
        "    if len(torch_img.shape) == 3 and (\n",
        "        torch_img.shape[0] == 3 or torch_img.shape[0] == 1\n",
        "    ):\n",
        "        return np.transpose(torch_img.detach().cpu().numpy(), (1, 2, 0))\n",
        "    elif len(torch_img.shape) == 2:\n",
        "        return torch_img.detach().cpu().numpy()\n",
        "    else:\n",
        "        raise ValueError(\"cannot process this image\")\n",
        "\n",
        "\n",
        "def np_img_to_torch_img(np_img):\n",
        "    \"\"\"Convert a np image to a torch image\n",
        "\n",
        "    Arguments:\n",
        "        np_img: a np.array of shape (B,H,W,C) or (H,W,C)\n",
        "    Returns:\n",
        "        a Tensor of shape (B,C,H,W) or (C,H,W)\n",
        "    Raises:\n",
        "        ValueError: If this is not a np.array\n",
        "    \"\"\"\n",
        "    if isinstance(np_img, torch.Tensor):\n",
        "        return np_img\n",
        "    assert isinstance(np_img, np.ndarray), \"cannot process data type: {0}\".format(\n",
        "        type(np_img)\n",
        "    )\n",
        "    if len(np_img.shape) == 4 and (np_img.shape[3] == 3 or np_img.shape[3] == 1):\n",
        "        return to_torch(np.transpose(np_img, (0, 3, 1, 2)))\n",
        "    if len(np_img.shape) == 3 and (np_img.shape[2] == 3 or np_img.shape[2] == 1):\n",
        "        return to_torch(np.transpose(np_img, (2, 0, 1)))\n",
        "    elif len(np_img.shape) == 2:\n",
        "        return to_torch(np_img)\n",
        "    else:\n",
        "        raise ValueError(\"cannot process this image\")\n",
        "\n",
        "\n",
        "def normalize_single_image_torch(image, img_mean=None, img_std=None):\n",
        "    \"\"\"Normalize a Torch tensor\n",
        "\n",
        "    Arguments:\n",
        "        image: Torch Tensor of shape (C,W,H)\n",
        "        img_mean: List of mean per channel (e.g.: [0.485, 0.456, 0.406])\n",
        "        img_std: List of std per channel (e.g.: [0.229, 0.224, 0.225])\n",
        "    Returns:\n",
        "        image: Torch Tensor of shape (C,W,H), the normalized image\n",
        "    Raises:\n",
        "        ValueError: If the shape of the image is not of lenth 3\n",
        "        ValueError: If the image is not a torch Tensor\n",
        "    \"\"\"\n",
        "    if len(image.shape) != 3:\n",
        "        raise ValueError(\n",
        "            \"The len(shape) of the image is {}, not 3\".format(len(image.shape))\n",
        "        )\n",
        "    if isinstance(image, torch.Tensor) == False:\n",
        "        raise ValueError(\"The image is not a torch Tensor\")\n",
        "    if img_mean is None and img_std is None:\n",
        "        img_mean = torch.mean(image, dim=(1, 2)).view(-1, 1, 1)\n",
        "        img_std = image.contiguous().view(image.size(0), -1).std(-1).view(-1, 1, 1)\n",
        "        image = (image - img_mean) / img_std\n",
        "    else:\n",
        "        image = Normalize(img_mean, img_std, inplace=False)(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def denormalize(x):\n",
        "    \"\"\"Scale image to range [0,1]\n",
        "\n",
        "    Arguments:\n",
        "        x: np.array, an image\n",
        "    Returns:\n",
        "        x: np.array, the scaled image\n",
        "    Raises:\n",
        "\n",
        "    \"\"\"\n",
        "    x_max = np.percentile(x, 98)\n",
        "    x_min = np.percentile(x, 2)\n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = x.clip(0, 1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "LO5VNUwbXWOK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"Builds the preprocessing function for each model. They all use torch/keras/gluoncv functions depending on the model.\n",
        "Arguments:\n",
        "    input_shape: Tuple of integer, the input_shape the model needs to take\n",
        "Returns:\n",
        "    preprocessing: function that takes an image as input, and returns the preprocessed image.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _build_keypoint_preprocessing(input_shape, backbone):\n",
        "    \"\"\"Builds the preprocessing function for the Field Keypoint Detector Model.\n",
        "\n",
        "    \"\"\"\n",
        "    sm_preprocessing = sm.get_preprocessing(backbone)\n",
        "\n",
        "    def preprocessing(input_img, **kwargs):\n",
        "\n",
        "        to_normalize = False if np.percentile(input_img, 98) > 1.0 else True\n",
        "\n",
        "        if len(input_img.shape) == 4:\n",
        "            print(\n",
        "                \"Only preprocessing single image, we will consider the first one of the batch\"\n",
        "            )\n",
        "            image = input_img[0] * 255.0 if to_normalize else input_img[0] * 1.0\n",
        "        else:\n",
        "            image = input_img * 255.0 if to_normalize else input_img * 1.0\n",
        "\n",
        "        image = cv2.resize(image, input_shape)\n",
        "        image = sm_preprocessing(image)\n",
        "        return image\n",
        "\n",
        "    return preprocessing\n",
        "\n",
        "\n",
        "\n",
        "def _build_homo_preprocessing(input_shape):\n",
        "    \"\"\"Builds the preprocessing function for the Deep Homography estimation Model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def preprocessing(input_img, **kwargs):\n",
        "\n",
        "        if len(input_img.shape) == 4:\n",
        "            print(\n",
        "                \"Only preprocessing single image, we will consider the first one of the batch\"\n",
        "            )\n",
        "            image = input_img[0]\n",
        "        else:\n",
        "            image = input_img\n",
        "\n",
        "        image = cv2.resize(image, input_shape)\n",
        "        image = torch_img_to_np_img(\n",
        "            normalize_single_image_torch(np_img_to_torch_img(image))\n",
        "        )\n",
        "        return image\n",
        "\n",
        "    return preprocessing\n"
      ],
      "metadata": {
        "id": "IKDzvG5wIyAZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def pyramid_layer(\n",
        "    x, indx, activation=\"tanh\", output_size=8, nb_neurons=[512, 512, 256, 128]\n",
        "):\n",
        "    \"\"\"Fully connected layers to add at the end of a network.\n",
        "\n",
        "    Arguments:\n",
        "        x: a tf.keras Tensor as input\n",
        "        indx: Integer, an index to add to the name of the layers\n",
        "        activation: String, name of the activation function to add at the end\n",
        "        output_size: Size of the last layer, number of outputs\n",
        "        nb_neurons: Size of the Dense layer to add\n",
        "    Returns:\n",
        "        output: a tf.keras Tensor as output\n",
        "    Raises:\n",
        "\n",
        "    \"\"\"\n",
        "    dense_name_base = \"full_\" + str(indx)\n",
        "    for indx, neuron in enumerate(nb_neurons):\n",
        "        x = tf.keras.layers.Dense(\n",
        "            neuron, name=dense_name_base + str(neuron) + \"_\" + str(indx)\n",
        "        )(x)\n",
        "    x = tf.keras.layers.Dense(output_size, name=dense_name_base + \"output\")(x)\n",
        "    output = tf.keras.layers.Activation(activation)(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "3nBwEbmFIZif"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1TTtiTGSIODc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def _build_resnet18():\n",
        "    \"\"\"Builds a resnet18 model in keras from a .h5 file.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    Returns:\n",
        "        a tf.keras.models.Model\n",
        "    Raises:\n",
        "    \"\"\"\n",
        "    resnet18_path_to_file = tf.keras.utils.get_file(\n",
        "        RESNET_ARCHI_TF_KERAS_NAME,\n",
        "        RESNET_ARCHI_TF_KERAS_PATH,\n",
        "        RESNET_ARCHI_TF_KERAS_TOTAR,\n",
        "    )\n",
        "\n",
        "    resnet18 = tf.keras.models.load_model(resnet18_path_to_file)\n",
        "    resnet18.compile()\n",
        "\n",
        "    inputs = resnet18.input\n",
        "    outputs = resnet18.layers[-2].output\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"custom_resnet18\")\n",
        "\n",
        "\n",
        "class DeepHomoModel:\n",
        "    \"\"\"Class for Keras Models to predict the corners displacement from an image. These corners can then get used\n",
        "    to compute the homography.\n",
        "\n",
        "    Arguments:\n",
        "        pretrained: Boolean, if the model is loaded pretrained on ImageNet or not\n",
        "        input_shape: Tuple, shape of the model's input\n",
        "    Call arguments:\n",
        "        input_img: a np.array of shape input_shape\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=False, input_shape=(256, 256)):\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.resnet_18 = _build_resnet18()\n",
        "\n",
        "        inputs = tf.keras.layers.Input((self.input_shape[0], self.input_shape[1], 3))\n",
        "        x = self.resnet_18(inputs)\n",
        "        outputs = pyramid_layer(x, 2)\n",
        "\n",
        "        self.model = tf.keras.models.Model(\n",
        "            inputs=[inputs], outputs=outputs, name=\"DeepHomoPyramidalFull\"\n",
        "        )\n",
        "\n",
        "        self.preprocessing = _build_homo_preprocessing(input_shape)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "\n",
        "        img = self.preprocessing(input_img)\n",
        "        corners = self.model.predict(np.array([img]))\n",
        "\n",
        "        return corners\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        try:\n",
        "            self.model.load_weights(weights_path)\n",
        "            print(\"Succesfully loaded weights from {}\".format(weights_path))\n",
        "        except:\n",
        "            orig_weights = \"Randomly\"\n",
        "            print(\n",
        "                \"Could not load weights from {}, weights will be loaded {}\".format(\n",
        "                    weights_path, orig_weights\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KeypointDetectorModel:\n",
        "    \"\"\"Class for Keras Models to predict the keypoint in an image. These keypoints can then be used to\n",
        "    compute the homography.\n",
        "\n",
        "    Arguments:\n",
        "        backbone: String, the backbone we want to use\n",
        "        model_choice: The model architecture. ('FPN','Unet','Linknet')\n",
        "        num_classes: Integer, number of mask to compute (= number of keypoints)\n",
        "        input_shape: Tuple, shape of the model's input\n",
        "    Call arguments:\n",
        "        input_img: a np.array of shape input_shape\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone=\"efficientnetb3\",\n",
        "        model_choice=\"FPN\",\n",
        "        num_classes=29,\n",
        "        input_shape=(320, 320),\n",
        "    ):\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.classes = [str(i) for i in range(num_classes)] + [\"background\"]\n",
        "        self.backbone = backbone\n",
        "\n",
        "        n_classes = len(self.classes)\n",
        "        activation = \"softmax\"\n",
        "\n",
        "        if model_choice == \"FPN\":\n",
        "            self.model = sm.FPN(\n",
        "                self.backbone,\n",
        "                classes=n_classes,\n",
        "                activation=activation,\n",
        "                input_shape=(input_shape[0], input_shape[1], 3),\n",
        "                encoder_weights=\"imagenet\",\n",
        "            )\n",
        "        else:\n",
        "            self.model = None\n",
        "            print(\"{} is not used yet\".format(model_choice))\n",
        "\n",
        "        self.preprocessing = _build_keypoint_preprocessing(input_shape, backbone)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "\n",
        "        img = self.preprocessing(input_img)\n",
        "        pr_mask = self.model.predict(np.array([img]))\n",
        "        return pr_mask\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        try:\n",
        "            self.model.load_weights(weights_path)\n",
        "            print(\"Succesfully loaded weights from {}\".format(weights_path))\n",
        "        except:\n",
        "            orig_weights = \"from Imagenet\"\n",
        "            print(\n",
        "                \"Could not load weights from {}, weights will be loaded {}\".format(\n",
        "                    weights_path, orig_weights\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "zP5TYNyEIRxa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepHomoModel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SJMS0Qv7Ygyx",
        "outputId": "3f33be7a-b39d-4d9e-ad06-887289bb9c39"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "EOF read where object expected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a64b70ad3a4f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepHomoModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-088238b500f4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained, input_shape)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_resnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-088238b500f4>\u001b[0m in \u001b[0;36m_build_resnet18\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/deep_homo_model_1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;31m# Legacy format deserialization (no \"module\" key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;31m# used for H5 and SavedModel formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             layer = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0minner_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             fn = python_utils.func_load(\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0minner_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mdefaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"defaults\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/python_utils.py\u001b[0m in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mraw_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_unicode_escape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mglobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: EOF read where object expected"
          ]
        }
      ]
    }
  ]
}